---
title: "RandomForest_GrpAsignment"
author: "Jai Kushwaha"
date: "08/12/2019"
output: word_document
---

```{r}
Thera_Bank = read_excel(file.choose())

 
library(randomForest)

## Missing value treatment 

str(Thera_Bank)
Thera_Bank$Familymembers[is.na(Thera_Bank$Familymembers)]=0
any(is.na(Thera_Bank)) ## no missing value
any(is.na(TBank.train)) ## no missing value
Thera_Bank$Experience_in_years [Thera_Bank$Experience_in_years<= 0.1 ]=0
summary(TBank.train)
## Negative values of experience changed to 0

Thera_Bank$random <- runif (nrow(Thera_Bank), 0, 1);
TBank.train <- Thera_Bank [which(Thera_Bank$random <= 0.7),]
Tbank.test <- Thera_Bank [which(Thera_Bank$random > 0.7),]

c (nrow(TBank.train), nrow(Tbank.test))
```

Dividing the dataset into Training and testing data set.

Also doing the missing value and -ve value reatment in the data.

```{r}
RF <- randomForest(as.factor(TBank.train$PersonalLoan) ~ ., data = TBank.train[,c(2:14)], 
                   ntree=501, mtry = 2, nodesize =10,
                   importance=TRUE)
print(RF)

```
Running random forest with trees and taking mtry as 
Out of bag error rate (OOB) =1.89%
Observation:
- From the cofusion matrix we can see False positive is 63 with is pretty high.
- Classification error For 1 i.e for loan given is pretty high at 19 %


```{r}
plot(RF, main="")
legend("topright", c("OOB", "0", "1"), text.col=1:6, lty=1:3, col=1:3)
title(main="Error Rates Random Forest Thera Bank Training Data")
```


Observation
- Between 70 to 80 trees the error for OOB 1 and 0 are decreasing sharply.
```{r}
RF$err.rate[,1]
print("Mini Error ")
min(RF$err.rate[,1])
```
Observation 
- Error of 0.18 is first attained likely around 171 tree.
```{r}
# Variable importance
impVar <- round(randomForest::importance(RF), 2)
impVar[order(impVar[,3], decreasing=TRUE),]
```


Observation:
- Based on MeanDecrease accuracy and GINI Income is the most important variable
- Zip code does not create sufficient on designing the model.

```{r}
## Tuning Random Forest
tRF <- tuneRF(x = TBank.train[,c(2,3,4,6,7,8,9,11,12,13,14)], 
              y=as.factor(TBank.train$PersonalLoan),
              mtryStart = 3, 
              ntreeTry=501, 
              stepFactor = 1.5, 
              improve = 0.00001, 
              trace=TRUE, 
              plot = TRUE,
              doBest = TRUE,
              nodesize = 10, 
              importance=TRUE
)
```

Obs:
1. OOB error decreased till mtry =6 then increased as evident from the above grapgh and data.
2. Optimum mtry is 6 
```{r}
#inserting class and prob score columns in training data:

TBank.train$predict.class <- predict(tRF, TBank.train, type="class")
TBank.train$predict.score <- predict(tRF, TBank.train, type="prob")

MLmetrics::AUC(TBank.train$predict.score[,2], TBank.train$PersonalLoan)


```

Obs:
- AUC is 99.97% for the traing data
```{r}
# Performance on Test data 

Tbank.test$predict.class <- predict(tRF, Tbank.test, type="class")
Tbank.test$predict.score <- predict(tRF, Tbank.test, type="prob")
MLmetrics::AUC(Tbank.test$predict.score[,2], 
               Tbank.test$PersonalLoan)
```
Obs:
- AUC for the test data is 99.83 %

```{r}
## Training dat performance through confusion matrix
table(TBank.train$predict.class,TBank.train$PersonalLoan)
str(TBank.train$predict.class)
str(TBank.train$PersonalLoan)
RFConf.Matx = confusionMatrix(TBank.train$predict.class, as.factor(TBank.train$PersonalLoan),positive = "1")
RFConf.Matx
```

```{r}
pred = prediction(TBank.train$predict.score[,2], TBank.train$PersonalLoan)

perf = performance(pred, "tpr" , "fpr")
plot(perf)
perf
```
```{r}
## Performance through  AUC, GINI and KS
auc = performance(pred, "auc")
auc = as.numeric(auc@y.values)
gini = ineq(TBank.train$predict.score[,2],type = "Gini")

decile <- function(x){
deciles <- vector(length=10)
for (i in seq(0.1,1,.1)){
  deciles[i*10] <- quantile(x, i, na.rm=T)
}
return (
  ifelse(x<deciles[1], 1,
         ifelse(x<deciles[2], 2,
                ifelse(x<deciles[3], 3,
                       ifelse(x<deciles[4], 4,
                              ifelse(x<deciles[5], 5,
                                     ifelse(x<deciles[6], 6,
                                            ifelse(x<deciles[7], 7,
                                                   ifelse(x<deciles[8], 8,
                                                          ifelse(x<deciles[9], 9, 10
                                                          ))))))))))
}


Tbank.test$deciles <- decile(Tbank.test$predict.score[,2])

tmp_DT = data.table(Tbank.test)
h_rank <- tmp_DT[, list(
  cnt = length(PersonalLoan), 
  cnt_resp = sum(PersonalLoan), 
  cnt_non_resp = sum(PersonalLoan == 0)) , 
  by=deciles][order(-deciles)]
h_rank$rrate <- round (h_rank$cnt_resp / h_rank$cnt,4);
h_rank$cum_resp <- cumsum(h_rank$cnt_resp)
h_rank$cum_non_resp <- cumsum(h_rank$cnt_non_resp)
h_rank$cum_rel_resp <- round(h_rank$cum_resp / sum(h_rank$cnt_resp),4);
h_rank$cum_rel_non_resp <- round(h_rank$cum_non_resp / sum(h_rank$cnt_non_resp),4);
h_rank$ks <- abs(h_rank$cum_rel_resp - h_rank$cum_rel_non_resp);

## Performance of Random Forest
RFConf.Matx
print ("Area under the curve for tuned random forest on test data")
auc
print("KS stat")
h_rank$ks
print("gini")
gini
```
```{r}
h_rank
```
Conclusion

Various types of models were attempted Some raw , some refined and tuned to display the their dissimilarity in approaching the same dataset under mostly similar conditions.

If given a choice between low OOB (out of bag) error and Accuracy . I will go with accuracy as this case demands so.

As financial institution we want to be more than 100% sure that there should be no tolerance for defaults and we are able to earn from interest income.

Model Name	   error%	     Classification Accuracy %
CART      	   0.21        
Random Forest	 1.89	       98.10
tuned Random   1.53	       99.41
Forest	

